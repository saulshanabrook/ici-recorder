{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T13:35:08.458042Z",
     "start_time": "2017-04-25T13:35:08.097754Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T13:35:08.473010Z",
     "start_time": "2017-04-25T13:35:08.459460Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config_schema = pyspark.sql.types.StructType.fromJson(\n",
    "{'fields': [\n",
    "  {'metadata': {},\n",
    "   'name': 'problem-file',\n",
    "   'nullable': True,\n",
    "   'type': 'string'},\n",
    "  {'metadata': {},\n",
    "   'name': 'argmap',\n",
    "   'nullable': True,\n",
    "   'type': {'keyType': 'string',\n",
    "    'type': 'map',\n",
    "    'valueContainsNull': True,\n",
    "    'valueType': 'string'}},\n",
    "  {'metadata': {},\n",
    "   'name': 'initialization-ms',\n",
    "   'nullable': True,\n",
    "   'type': 'long'},\n",
    "  {'metadata': {},\n",
    "   'name': 'registered-instructions',\n",
    "   'nullable': True,\n",
    "   'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n",
    "  {'metadata': {},\n",
    "   'name': 'version-number',\n",
    "   'nullable': True,\n",
    "   'type': 'string'},\n",
    "  {'metadata': {}, 'name': 'git-hash', 'nullable': True, 'type': 'string'},\n",
    "  {'metadata': {}, 'name': 'uuid', 'nullable': True, 'type': 'string'}],\n",
    " 'type': 'struct'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T13:35:08.788712Z",
     "start_time": "2017-04-25T13:35:08.590457Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generation_schema = pyspark.sql.types.StructType.fromJson(\n",
    "{'fields': [\n",
    "  {'name': 'error', 'nullable': True, 'type': 'string', 'metadata': {}},\n",
    "  {'metadata': {},\n",
    "   'name': 'outcome',\n",
    "   'nullable': True,\n",
    "   'type': 'string'},\n",
    "  {'metadata': {}, 'name': 'epsilons', 'nullable': True, 'type': 'double'},\n",
    "  {'metadata': {},\n",
    "   'name': 'population',\n",
    "   'nullable': True,\n",
    "   'type': {\n",
    "    'containsNull': True,\n",
    "    'elementType': {\n",
    "     'fields': [\n",
    "      {'metadata': {},\n",
    "       'name': 'genome',\n",
    "       'nullable': True,\n",
    "       'type': {\n",
    "        'containsNull': True,\n",
    "        'elementType': {\n",
    "         'fields': [\n",
    "          {'metadata': {},\n",
    "           'name': 'instruction',\n",
    "           'nullable': True,\n",
    "           'type': 'string'},\n",
    "          {'metadata': {}, 'name': 'uuid', 'nullable': True, 'type': 'string'},\n",
    "          {'metadata': {},\n",
    "           'name': 'random-insertion',\n",
    "           'nullable': True,\n",
    "           'type': 'boolean'},\n",
    "          {'metadata': {},\n",
    "           'name': 'silent',\n",
    "           'nullable': True,\n",
    "           'type': 'boolean'},\n",
    "          {'metadata': {},\n",
    "           'name': 'random-closes',\n",
    "           'nullable': True,\n",
    "           'type': 'integer'},\n",
    "          {'metadata': {},\n",
    "           'name': 'parent-uuid',\n",
    "           'nullable': True,\n",
    "           'type': 'string'}],\n",
    "         'type': 'struct'},\n",
    "        'type': 'array'}},\n",
    "      {'metadata': {}, 'name': 'program', 'nullable': True, 'type': 'string'},\n",
    "      {'metadata': {},\n",
    "       'name': 'errors',\n",
    "       'nullable': True,\n",
    "       'type': {'containsNull': True,\n",
    "        'elementType': 'double',\n",
    "        'type': 'array'}},\n",
    "      {'metadata': {},\n",
    "       'name': 'total-error',\n",
    "       'nullable': True,\n",
    "       'type': 'double'},\n",
    "      {'metadata': {},\n",
    "       'name': 'normalized-error',\n",
    "       'nullable': True,\n",
    "       'type': 'double'},\n",
    "      {'metadata': {},\n",
    "       'name': 'meta-errors',\n",
    "       'nullable': True,\n",
    "       'type': {'containsNull': True,\n",
    "        'elementType': 'double',\n",
    "        'type': 'array'}},\n",
    "      {'metadata': {},\n",
    "       'name': 'history',\n",
    "       'nullable': True,\n",
    "       'type': {'containsNull': True,\n",
    "        'elementType': 'double',\n",
    "        'type': 'array'}},\n",
    "      {'metadata': {},\n",
    "       'name': 'ancestors',\n",
    "       'nullable': True,\n",
    "       'type': {\n",
    "        'containsNull': True,\n",
    "        'elementType': {\n",
    "         'containsNull': True,\n",
    "         'elementType': {\n",
    "          'fields': [\n",
    "           {'metadata': {},\n",
    "            'name': 'instruction',\n",
    "            'nullable': True,\n",
    "            'type': 'string'},\n",
    "           {'metadata': {},\n",
    "            'name': 'uuid',\n",
    "            'nullable': True,\n",
    "            'type': 'string'},\n",
    "           {'metadata': {},\n",
    "            'name': 'random-insertion',\n",
    "            'nullable': True,\n",
    "            'type': 'boolean'},\n",
    "           {'metadata': {},\n",
    "            'name': 'silent',\n",
    "            'nullable': True,\n",
    "            'type': 'boolean'},\n",
    "           {'metadata': {},\n",
    "            'name': 'random-closes',\n",
    "            'nullable': True,\n",
    "            'type': 'integer'},\n",
    "           {'metadata': {},\n",
    "            'name': 'parent-uuid',\n",
    "            'nullable': True,\n",
    "            'type': 'string'}],\n",
    "          'type': 'struct'},\n",
    "         'type': 'array'},\n",
    "        'type': 'array'}},\n",
    "      {'metadata': {}, 'name': 'uuid', 'nullable': True, 'type': 'string'},\n",
    "      {'metadata': {},\n",
    "       'name': 'parent-uuids',\n",
    "       'nullable': True,\n",
    "       'type': {'containsNull': True,\n",
    "        'elementType': 'string',\n",
    "        'type': 'array'}},\n",
    "      {'metadata': {},\n",
    "       'name': 'genetic-operators',\n",
    "       'nullable': True,\n",
    "       'type': 'string'},\n",
    "      {'metadata': {},\n",
    "       'name': 'is-random-replacement',\n",
    "       'nullable': True,\n",
    "       'type': 'boolean'},\n",
    "      {'metadata': {}, 'name': 'age', 'nullable': True, 'type': 'integer'}],\n",
    "     'type': 'struct'},\n",
    "    'type': 'array'}},\n",
    "  {'metadata': {}, 'name': 'config-uuid', 'nullable': True, 'type': 'string'},\n",
    "  {'metadata': {}, 'name': 'index', 'nullable': True, 'type': 'integer'}],\n",
    " 'type': 'struct'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "     .master(\"local\") \\\n",
    "     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spark.sparkContext.setLogLevel(\"ALL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T13:37:55.186863Z",
     "start_time": "2017-04-25T13:37:55.150530Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configs_host, configs_port = os.environ['CONFIGS_HOST'].split(\":\")\n",
    "# generations_host, generations_port = os.environ['GENERATIONS_HOST'].split(\":\")\n",
    "\n",
    "# configs_str = spark \\\n",
    "#     .readStream \\\n",
    "#     .format(\"socket\") \\\n",
    "#     .option(\"host\", configs_host) \\\n",
    "#     .option(\"port\", int(configs_port)) \\\n",
    "#     .load()\n",
    "\n",
    "# generations_str = spark \\\n",
    "#     .readStream \\\n",
    "#     .format(\"socket\") \\\n",
    "#     .option(\"host\", generations_host) \\\n",
    "#     .option(\"port\", int(generations_port)) \\\n",
    "#     .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configs_str_query_console = configs_str \\\n",
    "#     .writeStream \\\n",
    "#     .format(\"console\") \\\n",
    "#     .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generations_str_query_console = generations_str \\\n",
    "#     .writeStream \\\n",
    "#     .format(\"console\") \\\n",
    "#     .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T13:37:59.942350Z",
     "start_time": "2017-04-25T13:37:59.815646Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configs = configs_str\\\n",
    "#     .select(\n",
    "#         pyspark.sql.functions.from_json(\n",
    "#             'value',\n",
    "#             config_schema\n",
    "#         ).alias('json')\n",
    "#     ) \\\n",
    "#     .select(\"json.*\")\n",
    "\n",
    "# generations = generations_str.select(\n",
    "#     pyspark.sql.functions.from_json(\n",
    "#         'value',\n",
    "#         generation_schema,\n",
    "#         options={\n",
    "#             \"columnNameOfCorruptRecord\": \"error\"\n",
    "#         }\n",
    "#     ).alias('json')\n",
    "# ).select(\"json.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: /configs;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o25.load.\n: org.apache.spark.sql.AnalysisException: Path does not exist: /configs;\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:215)\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:87)\n\tat org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:87)\n\tat org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:30)\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:124)\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:133)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-96851754b0ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgenerations_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'GENERATIONS_FOLDER'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mconfigs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mreadStream\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"json\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mgenerations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mreadStream\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"json\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    421\u001b[0m                 raise ValueError(\"If the path is provided for stream, it needs to be a \" +\n\u001b[1;32m    422\u001b[0m                                  \"non-empty string. List of paths are not supported.\")\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: /configs;'"
     ]
    }
   ],
   "source": [
    "configs_folder = os.environ['CONFIGS_FOLDER']\n",
    "generations_folder = os.environ['GENERATIONS_FOLDER']\n",
    "\n",
    "configs = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .load(configs_folder)\n",
    "\n",
    "generations = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .load(generations_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "configs = configs \\\n",
    "    .withColumn(\"label\", configs[\"argmap.label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T13:38:00.146335Z",
     "start_time": "2017-04-25T13:38:00.142919Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_uri = \"clojush/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T13:38:00.772189Z",
     "start_time": "2017-04-25T13:38:00.766792Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "configs_uri = base_uri + \"configs\"\n",
    "generations_uri = base_uri + \"generations\"\n",
    "\n",
    "configs_checkpoint_uri = base_uri + \"configs-checkpoint\"\n",
    "generations_checkpoint_uri = base_uri + \"generations-checkpoint\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T13:38:02.451885Z",
     "start_time": "2017-04-25T13:38:02.399167Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "configs_query_console = configs \\\n",
    "    .writeStream \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T13:36:50.918505Z",
     "start_time": "2017-04-25T13:36:50.871418Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generations_query_console = generations \\\n",
    "    .writeStream \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'isDataAvailable': False,\n",
       " 'isTriggerActive': False,\n",
       " 'message': 'Waiting for next trigger'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generations_query_console.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-25T13:28:46.013643Z",
     "start_time": "2017-04-25T13:28:45.928425Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "configs_query = configs \\\n",
    "    .writeStream \\\n",
    "    .start(\n",
    "        path=configs_uri,\n",
    "        format=\"parquet\",\n",
    "#         partitionBy=[\"label\"],\n",
    "        checkpointLocation=configs_checkpoint_uri,\n",
    "        queryName='configs'\n",
    "    )\n",
    "\n",
    "generations_query = generations \\\n",
    "    .writeStream \\\n",
    "    .start(\n",
    "        path=generations_uri,\n",
    "        format=\"parquet\",\n",
    "#         partitionBy=[\"config-uuid\", \"outcome\"],\n",
    "        checkpointLocation=generations_checkpoint_uri,\n",
    "        queryName='generations'\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- jsontostruct(value): struct (nullable = true)\n",
      " |    |-- outcome: string (nullable = false)\n",
      " |    |-- epsilons: double (nullable = true)\n",
      " |    |-- population: array (nullable = false)\n",
      " |    |    |-- element: struct (containsNull = false)\n",
      " |    |    |    |-- genome: array (nullable = false)\n",
      " |    |    |    |    |-- element: struct (containsNull = false)\n",
      " |    |    |    |    |    |-- instruction: string (nullable = false)\n",
      " |    |    |    |    |    |-- uuid: string (nullable = true)\n",
      " |    |    |    |    |    |-- random-insertion: boolean (nullable = true)\n",
      " |    |    |    |    |    |-- silent: boolean (nullable = true)\n",
      " |    |    |    |    |    |-- random-closes: integer (nullable = true)\n",
      " |    |    |    |    |    |-- parent-uuid: string (nullable = true)\n",
      " |    |    |    |-- program: string (nullable = true)\n",
      " |    |    |    |-- errors: array (nullable = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |    |-- total-error: double (nullable = true)\n",
      " |    |    |    |-- normalized-error: double (nullable = true)\n",
      " |    |    |    |-- meta-errors: array (nullable = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |    |-- history: array (nullable = true)\n",
      " |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |    |-- ancestors: array (nullable = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |    |-- instruction: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- uuid: string (nullable = true)\n",
      " |    |    |    |    |    |    |-- random-insertion: boolean (nullable = true)\n",
      " |    |    |    |    |    |    |-- silent: boolean (nullable = true)\n",
      " |    |    |    |    |    |    |-- random-closes: integer (nullable = true)\n",
      " |    |    |    |    |    |    |-- parent-uuid: string (nullable = true)\n",
      " |    |    |    |-- uuid: string (nullable = true)\n",
      " |    |    |    |-- parent-uuids: array (nullable = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- genetic-operators: string (nullable = true)\n",
      " |    |    |    |-- is-random-replacement: boolean (nullable = true)\n",
      " |    |    |    |-- age: integer (nullable = true)\n",
      " |    |-- config-uuid: string (nullable = false)\n",
      " |    |-- index: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generations_str.select(\n",
    "    pyspark.sql.functions.from_json(\n",
    "        'value',\n",
    "        generation_schema,\n",
    "        options={\n",
    "            \"columnNameOfCorruptRecord\": \"CORRUPT\",\n",
    "        }\n",
    "    )).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generations_query_console.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Other Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outcome</th>\n",
       "      <th>epsilons</th>\n",
       "      <th>population</th>\n",
       "      <th>config-uuid</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>continue</td>\n",
       "      <td>None</td>\n",
       "      <td>[([Row(instruction='exec_empty', uuid=None, ra...</td>\n",
       "      <td>37f84a71-c784-49a3-8451-32668da8801b</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>continue</td>\n",
       "      <td>None</td>\n",
       "      <td>[([Row(instruction='in1', uuid=None, random-in...</td>\n",
       "      <td>37f84a71-c784-49a3-8451-32668da8801b</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    outcome epsilons                                         population  \\\n",
       "0  continue     None  [([Row(instruction='exec_empty', uuid=None, ra...   \n",
       "1  continue     None  [([Row(instruction='in1', uuid=None, random-in...   \n",
       "2      None     None                                               None   \n",
       "\n",
       "                            config-uuid  index  \n",
       "0  37f84a71-c784-49a3-8451-32668da8801b    0.0  \n",
       "1  37f84a71-c784-49a3-8451-32668da8801b    1.0  \n",
       "2                                  None    NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(generations_uri).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem-file</th>\n",
       "      <th>argmap</th>\n",
       "      <th>initialization-ms</th>\n",
       "      <th>registered-instructions</th>\n",
       "      <th>version-number</th>\n",
       "      <th>git-hash</th>\n",
       "      <th>uuid</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clojush.problems.software.replace-space-with-n...</td>\n",
       "      <td>{'json-log-program-strings': 'false', 'epsilon...</td>\n",
       "      <td>None</td>\n",
       "      <td>[code_atom, genome_uniform_tag_mutation, code_...</td>\n",
       "      <td>2.30.0-1-SNAPSHOT</td>\n",
       "      <td>b4f94e2671a9e39b65058a6e136885a2f8a324fe</td>\n",
       "      <td>37f84a71-c784-49a3-8451-32668da8801b</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        problem-file  \\\n",
       "0  clojush.problems.software.replace-space-with-n...   \n",
       "\n",
       "                                              argmap initialization-ms  \\\n",
       "0  {'json-log-program-strings': 'false', 'epsilon...              None   \n",
       "\n",
       "                             registered-instructions     version-number  \\\n",
       "0  [code_atom, genome_uniform_tag_mutation, code_...  2.30.0-1-SNAPSHOT   \n",
       "\n",
       "                                   git-hash  \\\n",
       "0  b4f94e2671a9e39b65058a6e136885a2f8a324fe   \n",
       "\n",
       "                                   uuid label  \n",
       "0  37f84a71-c784-49a3-8451-32668da8801b  None  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(configs_uri).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T19:47:16.919675Z",
     "start_time": "2017-04-24T19:47:16.624547Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max_generations = generations.groupby('run-uuid').max('index')\n",
    "configs_with_max_gen = configs.join(\n",
    "    max_generations,\n",
    "    configs.uuid == max_generations[\"run-uuid\"],\n",
    "    \"leftouter\"\n",
    ").drop(\"run-uuid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T22:13:59.148925Z",
     "start_time": "2017-04-24T22:13:59.049701Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max_generations_new = generations\\\n",
    ".sort(\"run-uuid\", \"index\")\\\n",
    ".groupby(\"run-uuid\")\\\n",
    ".agg(\n",
    "    pyspark.sql.functions.last(\"population\").alias(\"last-population\"),\n",
    "    pyspark.sql.functions.last(\"index\").alias(\"last-index\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T22:30:38.292950Z",
     "start_time": "2017-04-24T22:30:38.187117Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "configs_with_last_generation = configs.join(\n",
    "    max_generations_new,\n",
    "    configs.uuid == max_generations_new[\"run-uuid\"],\n",
    "    \"leftouter\",\n",
    ").drop(\"run-uuid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T19:58:42.630441Z",
     "start_time": "2017-04-24T19:58:42.603694Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def mean(xs):\n",
    "    return sum(xs) / len(xs)\n",
    "\n",
    "def minimum_mean(errors):\n",
    "    \n",
    "    return mean(min(errors, key=mean))\n",
    "\n",
    "error_type = pyspark.sql.types.DoubleType\n",
    "errors_type = lambda: pyspark.sql.types.ArrayType(error_type())\n",
    "minimum_mean_udf = pyspark.sql.functions.udf(minimum_mean, error_type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T22:32:18.791474Z",
     "start_time": "2017-04-24T22:30:41.552625Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(uuid='39d5da45-6994-4bdc-a6ef-272c7f9969ea', label='amps', problem-file='clojush.problems.boolean.mux-6', last-generation=None, age-mediated-parent-selection='[0.05 0.5]', age-combining-function='proportionate', genetic-operator-probabilities='{:alternation 0.45, :uniform-mutation 0.45, :genesis 0.1}', age-mediated-parent-selection='[0.05 0.5]', label='amps')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs_with_last_generation \\\n",
    "    .select(\n",
    "        \"uuid\",\n",
    "        \"argmap.label\",\n",
    "        \"problem-file\",\n",
    "        (configs_with_last_generation['last-index'] + 1).alias(\"last-generation\"),\n",
    "        \"argmap.age-mediated-parent-selection\",\n",
    "        \"argmap.age-combining-function\",\n",
    "        \"argmap.genetic-operator-probabilities\",\n",
    "        \"argmap.age-mediated-parent-selection\",\n",
    "        \"argmap.label\",\n",
    "#         minimum_mean_udf(\"population.errors\").alias(\"best-fitness\")\n",
    "    ).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-24T21:43:20.113723Z",
     "start_time": "2017-04-24T21:43:20.019890Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"Invalid usage of '*' in expression 'explode';\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o163.select.\n: org.apache.spark.sql.AnalysisException: Invalid usage of '*' in expression 'explode';\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:40)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:57)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$expandStarExpression$1.applyOrElse(Analyzer.scala:681)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$expandStarExpression$1.applyOrElse(Analyzer.scala:657)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:310)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:310)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:309)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:331)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:188)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:329)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:307)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.expandStarExpression(Analyzer.scala:657)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$buildExpandedProjectList$1.apply(Analyzer.scala:642)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$buildExpandedProjectList$1.apply(Analyzer.scala:637)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveReferences$$buildExpandedProjectList(Analyzer.scala:637)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:555)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$9.applyOrElse(Analyzer.scala:550)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:61)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:60)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:550)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:487)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:62)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:50)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:63)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:2822)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1121)\n\tat sun.reflect.GeneratedMethodAccessor51.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-b89fa0790393>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconfigs_with_last_generation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"population.errors.*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"uuid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m         \"\"\"\n\u001b[0;32m--> 984\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    985\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"Invalid usage of '*' in expression 'explode';\""
     ]
    }
   ],
   "source": [
    "configs_with_last_generation.select(pyspark.sql.functions.explode(\"population.errors.*\"), \"uuid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
